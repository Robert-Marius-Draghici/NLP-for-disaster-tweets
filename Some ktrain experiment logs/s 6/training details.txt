most cleaning functions applied

MODEL_NAME = 'DeepPavlov/bert-base-cased-conversational'
t = text.Transformer(MODEL_NAME, maxlen=31, class_names=list(Y_train.unique()))
trn = t.preprocess_train(X_train.tolist(), Y_train.values)
val = t.preprocess_test(X_val.tolist(), Y_val.values)
model = t.get_classifier()
learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=64)

simulating training for different learning rates... this may take a few moments...
Train for 95 steps
Epoch 1/1024
95/95 [==============================] - 88s 923ms/step - loss: 0.6715 - accuracy: 0.5785
Epoch 2/1024
95/95 [==============================] - 71s 749ms/step - loss: 0.6586 - accuracy: 0.6077
Epoch 3/1024
95/95 [==============================] - 71s 748ms/step - loss: 0.6255 - accuracy: 0.6588
Epoch 4/1024
95/95 [==============================] - 71s 749ms/step - loss: 0.5144 - accuracy: 0.7712
Epoch 5/1024
95/95 [==============================] - 71s 747ms/step - loss: 0.4135 - accuracy: 0.8296
Epoch 6/1024
95/95 [==============================] - 71s 748ms/step - loss: 0.3428 - accuracy: 0.8651
Epoch 7/1024
95/95 [==============================] - 71s 748ms/step - loss: 0.3046 - accuracy: 0.8800
Epoch 8/1024
95/95 [==============================] - 71s 749ms/step - loss: 0.2829 - accuracy: 0.8943
Epoch 9/1024
95/95 [==============================] - 71s 748ms/step - loss: 0.3575 - accuracy: 0.8578
Epoch 10/1024
95/95 [==============================] - 71s 748ms/step - loss: 0.6410 - accuracy: 0.6201
Epoch 11/1024
95/95 [==============================] - 71s 747ms/step - loss: 0.7312 - accuracy: 0.5179
Epoch 12/1024
95/95 [==============================] - 71s 748ms/step - loss: 0.7976 - accuracy: 0.5267
Epoch 13/1024
31/95 [========>.....................] - ETA: 48s - loss: 1.4303 - accuracy: 0.5000

done.
Visually inspect loss plot and select learning rate associated with falling loss




begin training using onecycle policy with max lr of 2e-05...
Train for 96 steps, validate for 48 steps
Epoch 1/10
96/96 [==============================] - 84s 874ms/step - loss: 0.6364 - accuracy: 0.6333 - val_loss: 0.5636 - val_accuracy: 0.7374
Epoch 2/10
96/96 [==============================] - 79s 820ms/step - loss: 0.4770 - accuracy: 0.7903 - val_loss: 0.4159 - val_accuracy: 0.8201
Epoch 3/10
96/96 [==============================] - 79s 822ms/step - loss: 0.3745 - accuracy: 0.8425 - val_loss: 0.3984 - val_accuracy: 0.8273
Epoch 4/10
96/96 [==============================] - 79s 821ms/step - loss: 0.3104 - accuracy: 0.8791 - val_loss: 0.4445 - val_accuracy: 0.8168
Epoch 5/10
96/96 [==============================] - 79s 819ms/step - loss: 0.2465 - accuracy: 0.9126 - val_loss: 0.4561 - val_accuracy: 0.8253
Epoch 6/10
96/96 [==============================] - 79s 820ms/step - loss: 0.1723 - accuracy: 0.9412 - val_loss: 0.5610 - val_accuracy: 0.8168
Epoch 7/10
96/96 [==============================] - 79s 821ms/step - loss: 0.1149 - accuracy: 0.9631 - val_loss: 0.6011 - val_accuracy: 0.8116
Epoch 8/10
96/96 [==============================] - 79s 820ms/step - loss: 0.0832 - accuracy: 0.9734 - val_loss: 0.6571 - val_accuracy: 0.8221
Epoch 9/10
96/96 [==============================] - 79s 820ms/step - loss: 0.0675 - accuracy: 0.9793 - val_loss: 0.6879 - val_accuracy: 0.8109
Epoch 10/10
96/96 [==============================] - 79s 821ms/step - loss: 0.0586 - accuracy: 0.9806 - val_loss: 0.6968 - val_accuracy: 0.8109

<tensorflow.python.keras.callbacks.History at 0x7f9946c801d0>


              precision    recall  f1-score   support

           0       0.80      0.88      0.84       861
           1       0.82      0.72      0.77       662

    accuracy                           0.81      1523
   macro avg       0.81      0.80      0.80      1523
weighted avg       0.81      0.81      0.81      1523

array([[759, 102],
       [186, 476]])
